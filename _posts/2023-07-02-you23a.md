---
abstract: Decentralized partially observable Markov decision processes (Dec-POMDPs)
  formalize the problem of designing individual controllers for a group of collaborative
  agents under stochastic dynamics and partial observability. Seeking a global optimum
  is difficult (NEXP complete), but seeking a Nash equilibrium - each agent policy
  being a best response to the other agents - is more accessible, and allowed addressing
  infinite-horizon problems with solutions in the form of finite state controllers.
  In this paper, we show that this approach can be adapted to cases where only a generative
  model (a simulator) of the Dec-POMDP is available. This requires relying on a simulation-based
  POMDP solver to construct an agentâ€™s FSC node by node. A related process is used
  to heuristically derive initial FSCs. Experiment with benchmarks shows that MC-JESP
  is competitive with existing Dec-POMDP solvers, even better than many offline methods
  using explicit models.
openreview: dXz1Zcyeo-M
title: Monte-Carlo Search for an Equilibrium in Dec-POMDPs
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: you23a
month: 0
tex_title: Monte-{C}arlo Search for an Equilibrium in {Dec-POMDPs}
firstpage: 2444
lastpage: 2453
page: 2444-2453
order: 2444
cycles: false
bibtex_author: You, Yang and Thomas, Vincent and Colas, Francis and Buffet, Olivier
author:
- given: Yang
  family: You
- given: Vincent
  family: Thomas
- given: Francis
  family: Colas
- given: Olivier
  family: Buffet
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Nineth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/you23a/you23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
