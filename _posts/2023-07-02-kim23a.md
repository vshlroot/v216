---
abstract: 'For the stable optimization of deep neural networks, regularization methods
  such as dropout and batch normalization have been used in various tasks. Nevertheless,
  the correct position to apply dropout has rarely been discussed, and different positions
  have been employed depending on the practitioners. In this study, we investigate
  the correct position to apply dropout. We demonstrate that for a residual network
  with batch normalization, applying dropout at certain positions increases the performance,
  whereas applying dropout at other positions decreases the performance. Based on
  theoretical analysis, we provide the following guideline for the correct position
  to apply dropout: apply one dropout after the last batch normalization but before
  the last weight layer in the residual branch. We provide detailed theoretical explanations
  to support this claim and demonstrate them through module tests. In addition, we
  investigate the correct position of dropout in the head that produces the final
  prediction. Although the current consensus is to apply dropout after global average
  pooling, we prove that applying dropout before global average pooling leads to a
  more stable output. The proposed guidelines are validated through experiments using
  different datasets and models.'
openreview: 3RzSkpi6ra
title: How to use dropout correctly on residual networks with batch normalization
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim23a
month: 0
tex_title: How to use dropout correctly on residual networks with batch normalization
firstpage: 1058
lastpage: 1067
page: 1058-1067
order: 1058
cycles: false
bibtex_author: Kim, Bum Jun and Choi, Hyeyeon and Jang, Hyeonah and Lee, Donggeon
  and Kim, Sang Woo
author:
- given: Bum Jun
  family: Kim
- given: Hyeyeon
  family: Choi
- given: Hyeonah
  family: Jang
- given: Donggeon
  family: Lee
- given: Sang Woo
  family: Kim
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Nineth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/kim23a/kim23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/kim23a/kim23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
