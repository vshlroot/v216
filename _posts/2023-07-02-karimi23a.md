---
abstract: In the emerging paradigm of Federated Learning (FL), large amount of clients
  such as mobile devices are used to train possibly high-dimensional models on their
  respective data. Combining (dimension-wise) adaptive gradient methods (e.g., Adam,
  AMSGrad) with FL has been an active direction, which is shown to outperform traditional
  SGD based FL in many cases. In this paper, we focus on the problem of training federated
  deep neural networks, and propose a novel FL framework which further introduces
  layer-wise adaptivity to the local model updates to accelerate the convergence of
  adaptive FL methods. Our framework includes two variants based on two recent locally
  adaptive federated learning algorithms. Theoretically, we provide a convergence
  analysis of our layer-wise FL methods, coined Fed-LAMB and Mime-LAMB, which match
  the convergence rate of state-of-the-art results in adaptive FL and exhibits linear
  speedup in terms of the number of workers. Experimental results on various datasets
  and models, under both IID and non-IID local data settings, show that both Fed-LAMB
  and Mime-LAMB achieve faster convergence speed and better generalization performance,
  compared to various recent adaptive FL methods.
openreview: G-tRlFNpEe
title: 'Fed-LAMB: Layer-wise and Dimension-wise Locally Adaptive Federated Learning'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: karimi23a
month: 0
tex_title: "{Fed-LAMB}: Layer-wise and Dimension-wise Locally Adaptive Federated Learning"
firstpage: 1037
lastpage: 1046
page: 1037-1046
order: 1037
cycles: false
bibtex_author: Karimi, Belhal and Li, Ping and Li, Xiaoyun
author:
- given: Belhal
  family: Karimi
- given: Ping
  family: Li
- given: Xiaoyun
  family: Li
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Nineth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/karimi23a/karimi23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/karimi23a/karimi23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
