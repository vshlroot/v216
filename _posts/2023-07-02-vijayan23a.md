---
abstract: We propose policy gradient algorithms for solving a risk-sensitive reinforcement
  learning (RL) problem in on-policy as well as off-policy settings. We consider episodic
  Markov decision processes, and model the risk using the broad class of smooth risk
  measures of the cumulative discounted reward. We propose two template policy gradient
  algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings,
  respectively. We derive non-asymptotic bounds that quantify the rate of convergence
  of our proposed algorithms to a stationary point of the smooth risk measure. As
  special cases, we establish that our algorithms apply to optimization of mean-variance
  and distortion risk measures, respectively.
openreview: a7PmdX41IQ
title: A policy gradient approach for optimization of smooth risk measures
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vijayan23a
month: 0
tex_title: A policy gradient approach for optimization of smooth risk measures
firstpage: 2168
lastpage: 2178
page: 2168-2178
order: 2168
cycles: false
bibtex_author: Vijayan, Nithia and Prashanth, L. A.
author:
- given: Nithia
  family: Vijayan
- given: L. A.
  family: Prashanth
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Nineth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/vijayan23a/vijayan23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/vijayan23a/vijayan23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
