---
abstract: Cutset networks (CNs) are interpretable probabilistic representations that
  combine probability trees and tree Bayesian networks, to model and reason about
  large multi-dimensional probability distributions. Motivated by high-stakes applications
  in domains such as healthcare where (a) rich domain knowledge in the form of qualitative
  influences is readily available and (b) use of interpretable models that the user
  can efficiently probe and infer over is often necessary, we focus on learning CNs
  in the presence of qualitative influences. We propose a penalized objective function
  that uses the influences as constraints, and develop a gradient-based learning algorithm,
  KICN. We show that because CNs are tractable, KICN is guaranteed to converge to
  a local maximum of the penalized objective function. Our experiments on several
  benchmark data sets show that our new algorithm is superior to the state-of-the-art,
  especially when the data is scarce or noisy.
openreview: YZpWAfSCt6
title: Knowledge Intensive Learning of Cutset Networks
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mathur23a
month: 0
tex_title: Knowledge Intensive Learning of Cutset Networks
firstpage: 1380
lastpage: 1389
page: 1380-1389
order: 1380
cycles: false
bibtex_author: Mathur, Saurabh and Gogate, Vibhav and Natarajan, Sriraam
author:
- given: Saurabh
  family: Mathur
- given: Vibhav
  family: Gogate
- given: Sriraam
  family: Natarajan
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Nineth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/mathur23a/mathur23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/mathur23a/mathur23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
