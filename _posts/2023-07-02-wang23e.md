---
abstract: Knowledge distillation is a popular technique that has been shown to produce
  remarkable gains in average accuracy. However, recent work has shown that these
  gains are not uniform across subgroups in the data, and can often come at the cost
  of accuracy on rare subgroups and classes. Robust optimization is a common remedy
  to improve worst-class accuracy in standard learning settings, but in distillation
  it is unknown whether it is best to apply robust objectives when training the teacher,
  the student, or both. This work studies the interplay between robust objectives
  for the teacher and student. Empirically, we show that that jointly modifying the
  teacher and student objectives can lead to better worst-class student performance
  and even Pareto improvement in the trade-off between worst-class and overall performance.
  Theoretically, we show that the <em>per-class calibration</em> of teacher scores
  is key when training a robust student. Both the theory and experiments support the
  surprising finding that applying a robust teacher training objective does not always
  yield a more robust student.
openreview: K2SnnO8NuP
title: 'Robust distillation for worst-class performance: on the interplay between
  teacher and student objectives'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23e
month: 0
tex_title: 'Robust distillation for worst-class performance: on the interplay between
  teacher and student objectives'
firstpage: 2237
lastpage: 2247
page: 2237-2247
order: 2237
cycles: false
bibtex_author: Wang, Serena and Narasimhan, Harikrishna and Zhou, Yichen and Hooker,
  Sara and Lukasik, Michal and Menon, Aditya Krishna
author:
- given: Serena
  family: Wang
- given: Harikrishna
  family: Narasimhan
- given: Yichen
  family: Zhou
- given: Sara
  family: Hooker
- given: Michal
  family: Lukasik
- given: Aditya Krishna
  family: Menon
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Nineth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/wang23e/wang23e.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/wang23e/wang23e-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
